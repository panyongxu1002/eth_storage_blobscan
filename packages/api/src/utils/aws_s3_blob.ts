// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

// snippet-start:[javascript.v3.s3.hello]
import {
  ListBucketsCommand,
  S3Client,
  PutObjectCommand,
} from "@aws-sdk/client-s3";
import fs from "fs";
import { fileURLToPath } from "url";

const config: any = {
  region: process.env.AWS_S3_STORAGE_REGION, // æ›¿æ¢ä¸ºä½ çš„AWSåŒºåŸŸ
  credentials: {
    accessKeyId: process.env.AWS_S3_STORAGE_KEY_ID, // æ›¿æ¢ä¸ºä½ çš„AWSè®¿é—®å¯†é’¥ID
    secretAccessKey: process.env.AWS_S3_SERVICE_KEY, // æ›¿æ¢ä¸ºä½ çš„AWSç§˜å¯†è®¿é—®å¯†é’¥
  },
};
// é…ç½® AWS è®¤è¯ä¿¡æ¯
const s3Client = new S3Client(config);

// æ‰§è¡Œä¸Šä¼ æ“ä½œ
export const upload_AWS_S3 = async ({ versionedHash, data }: any) => {
  // åˆ›å»ºä¸€ä¸ªå¯è¯»æµæ¥è¯»å–æ–‡ä»¶
  const jsonString = JSON.stringify(data);
  // console.log("ğŸš€ ~ constupload_AWS_S3= ~ jsonString:", jsonString);
  // å†™å…¥ JSON å­—ç¬¦ä¸²åˆ°æ–‡ä»¶
  fs.writeFileSync(`${versionedHash}.txt`, jsonString);

  const fileStream = fs.createReadStream(`${versionedHash}.txt`);

  // console.log("ğŸš€ ~ constupload_AWS_S3= ~ fileStream:", fileStream);
  // è®¾ç½® S3 ä¸­çš„æ–‡ä»¶åç§°å’Œè·¯å¾„
  const uploadParams = {
    Bucket: process.env.AWS_S3_STORAGE_BUCKET_NAME, // æ›¿æ¢ä¸ºä½ çš„S3å­˜å‚¨æ¡¶åç§°
    Key: `${versionedHash}.txt`, // æ–‡ä»¶ååŠè·¯å¾„
    Body: fileStream, // æ–‡ä»¶æµ
  };
  // console.log("ğŸš€ ~ constupload_AWS_S3= ~ uploadParams:", uploadParams);
  try {
    const data = await s3Client.send(new PutObjectCommand(uploadParams));
    console.log("File uploaded successfully. File location:", data.ETag);
  } catch (err) {
    console.error("Error uploading file:", err);
  } finally {
    // å…³é—­æ–‡ä»¶æµ
    fileStream.close();
    // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    fs.unlinkSync(`${versionedHash}.txt`);
  }
};
